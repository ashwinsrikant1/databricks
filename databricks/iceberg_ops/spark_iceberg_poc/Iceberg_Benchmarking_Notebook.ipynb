{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Table Benchmarking System\n",
    "\n",
    "This notebook provides a complete interface for creating and benchmarking Iceberg tables in Databricks.\n",
    "\n",
    "## Features\n",
    "- **Dataset Generation**: Create tables with configurable size and schema\n",
    "- **Write Performance**: Measure table creation and insert throughput\n",
    "- **Read Benchmarking**: Run comprehensive query performance tests\n",
    "- **Configuration Management**: Easily modify parameters through config variables\n",
    "\n",
    "## Quick Start\n",
    "1. Modify configuration variables in the \"Configuration\" section\n",
    "2. Run \"Create Dataset\" to generate a new table\n",
    "3. Run \"Benchmark Queries\" to test read performance\n",
    "4. View detailed performance metrics and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nimport json\nfrom datetime import datetime\n\n# Add databricks-utils to path\nsys.path.append(os.path.join(os.getcwd(), '..', '..', 'databricks-utils'))\n\n# Import our modules with the new schema-driven functions\nfrom dataset_generator import (\n    create_table_by_rows, create_table_by_size, \n    create_table_with_custom_schema, DEFAULT_SCHEMAS,\n    SUPPORTED_TYPES\n)\nfrom benchmark_queries import run_standard_benchmark\nfrom cluster_execution import get_e2_demo_client, get_cluster_info\n\nprint(\"âœ“ All modules imported successfully!\")\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Display available schema types and data types\nprint(\"\\nðŸ“‹ Available Schema Types:\")\nfor name, config in DEFAULT_SCHEMAS.items():\n    col_summary = ', '.join([f'{col[\"name\"]}:{col[\"type\"]}' for col in config])\n    print(f\"  {name}: {col_summary}\")\n\nprint(f\"\\nðŸ”§ Supported Data Types:\")\ntype_names = list(SUPPORTED_TYPES.keys())\nfor i in range(0, len(type_names), 4):  # Print 4 types per line\n    line_types = type_names[i:i+4]\n    print(f\"  {', '.join(line_types)}\")\n\nprint(f\"\\nTotal: {len(type_names)} data types supported, including VARIANT for JSON-like data\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Modify these variables to control your benchmarking setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION VARIABLES\n",
    "# ========================================\n",
    "\n",
    "# Experiment Configuration\n",
    "RUN_NAME = \"schema_test\"  # Name for this benchmarking run (used for organizing results)\n",
    "\n",
    "# Database Configuration\n",
    "SCHEMA = \"users.ashwin_srikant\"  # Target schema for all tables\n",
    "TABLE_NAME = \"iceberg_schema_benchmark_test\"  # Table name for this session\n",
    "\n",
    "# Dataset Generation Method (choose one)\n",
    "USE_ROW_COUNT = True  # Set to True to use row count, False to use target size\n",
    "\n",
    "# Row-based configuration\n",
    "NUM_ROWS = 1000  # Number of rows to generate\n",
    "\n",
    "# Size-based configuration (used if USE_ROW_COUNT = False)\n",
    "TARGET_SIZE_GB = 1.0  # Target table size in GB\n",
    "\n",
    "# Schema Configuration - Choose a predefined schema or define custom\n",
    "SCHEMA_TYPE = \"mixed_types\"  # Options: 'simple', 'mixed_types', 'with_variant', 'custom'\n",
    "\n",
    "# Custom Schema Configuration (used when SCHEMA_TYPE = 'custom')\n",
    "# Define your own table schema as a list of column dictionaries\n",
    "CUSTOM_SCHEMA = [\n",
    "    {'name': 'id', 'type': 'bigint', 'primary_key': True},\n",
    "    {'name': 'user_name', 'type': 'string', 'length': 50},\n",
    "    {'name': 'email', 'type': 'string', 'length': 100},\n",
    "    {'name': 'age', 'type': 'int', 'min_value': 18, 'max_value': 100},\n",
    "    {'name': 'balance', 'type': 'decimal', 'precision': 10, 'scale': 2, 'min_value': 0, 'max_value': 100000},\n",
    "    {'name': 'is_premium', 'type': 'boolean'},\n",
    "    {'name': 'signup_date', 'type': 'date'},\n",
    "    {'name': 'last_activity', 'type': 'timestamp'},\n",
    "    {'name': 'profile_data', 'type': 'variant'},  # JSON-like data\n",
    "    {'name': 'notes', 'type': 'string', 'length': 500}\n",
    "]\n",
    "\n",
    "# Cluster Configuration\n",
    "CLUSTER_ID = \"0819-033442-njp866rg\"  # Use your cluster ID here\n",
    "\n",
    "# Import the schema configurations from dataset_generator\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', '..', 'databricks-utils'))\n",
    "from dataset_generator import DEFAULT_SCHEMAS\n",
    "\n",
    "# Display current configuration\n",
    "print(\"ðŸ“‹ CURRENT CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Run Name: {RUN_NAME}\")\n",
    "print(f\"Schema: {SCHEMA}\")\n",
    "print(f\"Table: {TABLE_NAME}\")\n",
    "print(f\"Cluster ID: {CLUSTER_ID}\")\n",
    "print()\n",
    "\n",
    "# Display selected schema configuration\n",
    "if SCHEMA_TYPE == 'custom':\n",
    "    selected_schema = CUSTOM_SCHEMA\n",
    "    print(f\"Schema Type: Custom ({len(selected_schema)} columns)\")\n",
    "else:\n",
    "    selected_schema = DEFAULT_SCHEMAS.get(SCHEMA_TYPE, DEFAULT_SCHEMAS['simple'])\n",
    "    print(f\"Schema Type: {SCHEMA_TYPE} ({len(selected_schema)} columns)\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ“Š TABLE SCHEMA:\")\n",
    "for i, col in enumerate(selected_schema, 1):\n",
    "    extra_info = []\n",
    "    if 'length' in col:\n",
    "        extra_info.append(f\"length={col['length']}\")\n",
    "    if 'min_value' in col or 'max_value' in col:\n",
    "        if 'min_value' in col and 'max_value' in col:\n",
    "            extra_info.append(f\"range={col['min_value']}-{col['max_value']}\")\n",
    "        elif 'min_value' in col:\n",
    "            extra_info.append(f\"min={col['min_value']}\")\n",
    "        elif 'max_value' in col:\n",
    "            extra_info.append(f\"max={col['max_value']}\")\n",
    "    if col.get('primary_key'):\n",
    "        extra_info.append(\"PRIMARY KEY\")\n",
    "    \n",
    "    extra_str = f\" ({', '.join(extra_info)})\" if extra_info else \"\"\n",
    "    print(f\"  {i:2}. {col['name']:<15} {col['type'].upper():<10} {extra_str}\")\n",
    "\n",
    "print()\n",
    "if USE_ROW_COUNT:\n",
    "    print(f\"Mode: Row count based\")\n",
    "    print(f\"Rows: {NUM_ROWS:,}\")\n",
    "    # Estimate size based on schema\n",
    "    estimated_row_size = sum(\n",
    "        col.get('length', 100) if col['type'] in ['string', 'varchar'] \n",
    "        else 200 if col['type'] == 'variant'\n",
    "        else 50 \n",
    "        for col in selected_schema\n",
    "    )\n",
    "    estimated_size_mb = (NUM_ROWS * estimated_row_size) / (1024 * 1024)\n",
    "    print(f\"Estimated row size: {estimated_row_size} bytes\")\n",
    "    print(f\"Estimated total size: {estimated_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"Mode: Size based\")\n",
    "    print(f\"Target size: {TARGET_SIZE_GB} GB\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Store the selected schema for use in other cells\n",
    "SELECTED_SCHEMA_CONFIG = selected_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Databricks Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Databricks connection\n",
    "try:\n",
    "    client = get_e2_demo_client()\n",
    "    print(\"âœ“ Databricks connection established successfully!\")\n",
    "    print(f\"Connected to: {os.getenv('DATABRICKS_HOST')}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to connect to Databricks: {e}\")\n",
    "    print(\"Please check your E2_DEMO_FIELD_ENG_PAT environment variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Configuration\n",
    "\n",
    "Display detailed information about the compute cluster being used for benchmarking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display cluster configuration\n",
    "try:\n",
    "    print(\"ðŸ–¥ï¸  Retrieving cluster configuration...\")\n",
    "    cluster_info = get_cluster_info(client, CLUSTER_ID)\n",
    "    \n",
    "    if 'error' not in cluster_info:\n",
    "        print(\"\\nðŸ“Š CLUSTER CONFIGURATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Display key cluster information in a formatted way\n",
    "        print(f\"Cluster Name:        {cluster_info['cluster_name']}\")\n",
    "        print(f\"Cluster ID:          {cluster_info['cluster_id']}\")\n",
    "        print(f\"State:               {cluster_info['state']}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"ðŸ’» Compute Configuration:\")\n",
    "        print(f\"  Node Type:         {cluster_info['node_type_id']}\")\n",
    "        print(f\"  Driver Node:       {cluster_info['driver_node_type_id']}\")\n",
    "        print(f\"  Workers:           {cluster_info['num_workers']}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"âš™ï¸  Runtime Configuration:\")\n",
    "        print(f\"  Spark Version:     {cluster_info['spark_version']}\")\n",
    "        print(f\"  Runtime Engine:    {cluster_info['runtime_engine']}\")\n",
    "        print(f\"  Security Mode:     {cluster_info['data_security_mode']}\")\n",
    "        print(f\"  Auto-termination:  {cluster_info['autotermination_minutes']} minutes\")\n",
    "        \n",
    "        # Display AWS-specific info if available\n",
    "        if 'aws_zone_id' in cluster_info:\n",
    "            print()\n",
    "            print(\"â˜ï¸  AWS Configuration:\")\n",
    "            print(f\"  Availability Zone: {cluster_info['aws_zone_id']}\")\n",
    "            if cluster_info.get('aws_instance_profile_arn'):\n",
    "                print(f\"  Instance Profile:  {cluster_info['aws_instance_profile_arn']}\")\n",
    "        \n",
    "        # Display autoscaling info if available\n",
    "        if 'autoscale_min_workers' in cluster_info:\n",
    "            print()\n",
    "            print(\"ðŸ“ˆ Autoscaling Configuration:\")\n",
    "            print(f\"  Min Workers:       {cluster_info['autoscale_min_workers']}\")\n",
    "            print(f\"  Max Workers:       {cluster_info['autoscale_max_workers']}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Store cluster info for session summary\n",
    "        cluster_configuration = cluster_info\n",
    "        \n",
    "    else:\n",
    "        print(f\"âŒ Failed to retrieve cluster info: {cluster_info['error']}\")\n",
    "        cluster_configuration = None\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error getting cluster configuration: {e}\")\n",
    "    cluster_configuration = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time for session tracking\n",
    "session_start = datetime.now()\n",
    "print(f\"ðŸš€ Starting dataset creation at {session_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Using schema: {SCHEMA_TYPE}\")\n",
    "print()\n",
    "\n",
    "# Create dataset based on configuration using the new schema-driven approach\n",
    "try:\n",
    "    if USE_ROW_COUNT:\n",
    "        print(f\"Creating table with {NUM_ROWS:,} rows using '{SCHEMA_TYPE}' schema...\")\n",
    "        write_performance = create_table_with_custom_schema(\n",
    "            table_name=TABLE_NAME,\n",
    "            schema_config=SELECTED_SCHEMA_CONFIG,\n",
    "            num_rows=NUM_ROWS,\n",
    "            database_schema=SCHEMA\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Creating table with target size {TARGET_SIZE_GB} GB using '{SCHEMA_TYPE}' schema...\")\n",
    "        write_performance = create_table_by_size(\n",
    "            table_name=TABLE_NAME,\n",
    "            target_size_gb=TARGET_SIZE_GB,\n",
    "            schema_config=SELECTED_SCHEMA_CONFIG,\n",
    "            database_schema=SCHEMA\n",
    "        )\n",
    "    \n",
    "    print(\"\\nâœ… Dataset creation completed successfully!\")\n",
    "    \n",
    "    # Store results for later reference\n",
    "    write_results = write_performance\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    write_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Performance Analysis\n",
    "\n",
    "Detailed analysis of table creation and insert performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_results:\n",
    "    print(\"ðŸ“Š WRITE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Key metrics\n",
    "    total_time = write_results['total_time_seconds']\n",
    "    insert_time = write_results['total_insert_time_seconds']\n",
    "    text_gen_time = write_results['text_generation_time_seconds']\n",
    "    \n",
    "    print(f\"Table: {write_results['table_name']}\")\n",
    "    print(f\"Rows: {write_results['num_rows']:,}\")\n",
    "    print(f\"Data size: {write_results['total_data_size_mb']:.2f} MB\")\n",
    "    print()\n",
    "    print(\"â±ï¸  Performance Breakdown:\")\n",
    "    print(f\"  Total time:        {total_time:8.3f}s (100.0%)\")\n",
    "    print(f\"  Insert operations: {insert_time:8.3f}s ({(insert_time/total_time)*100:5.1f}%)\")\n",
    "    print(f\"  Text generation:   {text_gen_time:8.3f}s ({(text_gen_time/total_time)*100:5.1f}%)\")\n",
    "    print()\n",
    "    print(\"ðŸš€ Throughput Metrics:\")\n",
    "    print(f\"  Overall:     {write_results['overall_rows_per_second']:8.1f} rows/s | {write_results['overall_mb_per_second']:8.2f} MB/s\")\n",
    "    print(f\"  Insert only: {write_results['insert_rows_per_second']:8.1f} rows/s | {write_results['insert_mb_per_second']:8.2f} MB/s\")\n",
    "    \n",
    "    # Table statistics\n",
    "    if 'table_stats' in write_results:\n",
    "        stats = write_results['table_stats']\n",
    "        print()\n",
    "        print(\"ðŸ“ˆ Final Table Statistics:\")\n",
    "        print(f\"  {stats}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"âŒ No write performance data available. Please run dataset creation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Read Performance Benchmarks\n",
    "\n",
    "Execute a comprehensive suite of read queries to measure query performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive read benchmarks\n",
    "try:\n",
    "    print(f\"ðŸ” Starting read performance benchmarks on {SCHEMA}.{TABLE_NAME}\")\n",
    "    print()\n",
    "    \n",
    "    benchmark_results = run_standard_benchmark(TABLE_NAME, SCHEMA)\n",
    "    \n",
    "    print(\"\\nâœ… Read benchmarks completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to run benchmarks: {e}\")\n",
    "    benchmark_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Performance Analysis\n",
    "\n",
    "Detailed analysis of query performance results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if benchmark_results:\n",
    "    print(\"ðŸ“Š READ PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Separate successful and failed queries\n",
    "    successful = [r for r in benchmark_results if r[\"status\"] == \"success\"]\n",
    "    failed = [r for r in benchmark_results if r[\"status\"] == \"error\"]\n",
    "    \n",
    "    print(f\"ðŸ“ˆ Summary:\")\n",
    "    print(f\"  Total queries: {len(benchmark_results)}\")\n",
    "    print(f\"  Successful: {len(successful)}\")\n",
    "    print(f\"  Failed: {len(failed)}\")\n",
    "    \n",
    "    if successful:\n",
    "        total_time = sum(r[\"execution_time_seconds\"] for r in successful)\n",
    "        avg_time = total_time / len(successful)\n",
    "        min_time = min(r[\"execution_time_seconds\"] for r in successful)\n",
    "        max_time = max(r[\"execution_time_seconds\"] for r in successful)\n",
    "        \n",
    "        print(f\"  Total execution time: {total_time:.3f}s\")\n",
    "        print(f\"  Average query time: {avg_time:.3f}s\")\n",
    "        print(f\"  Fastest query: {min_time:.3f}s\")\n",
    "        print(f\"  Slowest query: {max_time:.3f}s\")\n",
    "        \n",
    "        print()\n",
    "        print(\"âš¡ Query Performance Breakdown:\")\n",
    "        print(f\"{'Query Name':<35} {'Time (s)':<10} {'Status':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for result in benchmark_results:\n",
    "            status_icon = \"âœ“\" if result['status'] == 'success' else \"âœ—\"\n",
    "            print(f\"{result['query_name']:<35} {result['execution_time_seconds']:<10.3f} {status_icon} {result['status']}\")\n",
    "    \n",
    "    if failed:\n",
    "        print()\n",
    "        print(\"âŒ Failed Queries:\")\n",
    "        for result in failed:\n",
    "            print(f\"  {result['query_name']}: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"âŒ No benchmark results available. Please run read benchmarks first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Summary\n",
    "\n",
    "Complete overview of this benchmarking session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_end = datetime.now()\n",
    "session_duration = session_end - session_start\n",
    "\n",
    "print(\"ðŸŽ¯ BENCHMARKING SESSION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Run Name: {RUN_NAME}\")\n",
    "print(f\"Session started: {session_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Session ended:   {session_end.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total duration:  {session_duration}\")\n",
    "print()\n",
    "\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "print(f\"  Table: {SCHEMA}.{TABLE_NAME}\")\n",
    "print(f\"  Cluster: {CLUSTER_ID}\")\n",
    "if USE_ROW_COUNT:\n",
    "    print(f\"  Rows: {NUM_ROWS:,}\")\n",
    "    print(f\"  Text length: {TEXT_LENGTH:,} bytes\")\n",
    "else:\n",
    "    print(f\"  Target size: {TARGET_SIZE_GB} GB\")\n",
    "\n",
    "# Include cluster configuration in summary\n",
    "if 'cluster_configuration' in locals() and cluster_configuration and 'error' not in cluster_configuration:\n",
    "    print()\n",
    "    print(f\"ðŸ–¥ï¸  Cluster Details:\")\n",
    "    print(f\"  Node Type: {cluster_configuration['node_type_id']}\")\n",
    "    print(f\"  Workers: {cluster_configuration['num_workers']}\")\n",
    "    print(f\"  Runtime: {cluster_configuration['runtime_engine']}\")\n",
    "    print(f\"  Spark Version: {cluster_configuration['spark_version']}\")\n",
    "\n",
    "print()\n",
    "if 'write_results' in locals() and write_results:\n",
    "    print(f\"ðŸ“ Write Performance:\")\n",
    "    print(f\"  Created {write_results['num_rows']:,} rows in {write_results['total_time_seconds']:.1f}s\")\n",
    "    print(f\"  Throughput: {write_results['overall_rows_per_second']:.1f} rows/s\")\n",
    "    print(f\"  Data size: {write_results['total_data_size_mb']:.2f} MB\")\n",
    "\n",
    "print()\n",
    "if 'benchmark_results' in locals() and benchmark_results:\n",
    "    successful_reads = [r for r in benchmark_results if r[\"status\"] == \"success\"]\n",
    "    print(f\"ðŸ“– Read Performance:\")\n",
    "    print(f\"  Executed {len(benchmark_results)} queries ({len(successful_reads)} successful)\")\n",
    "    if successful_reads:\n",
    "        avg_read_time = sum(r[\"execution_time_seconds\"] for r in successful_reads) / len(successful_reads)\n",
    "        print(f\"  Average query time: {avg_read_time:.3f}s\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ Session completed successfully!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results (Optional)\n",
    "\n",
    "Save detailed results to JSON files for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON files\n",
    "export_results = input(\"Export results to JSON files? (y/N): \").lower().strip() == 'y'\n",
    "\n",
    "if export_results:\n",
    "    # Create organized directory structure\n",
    "    timestamp = session_start.strftime('%Y%m%d_%H%M%S')\n",
    "    results_dir = f\"results/{RUN_NAME}_{timestamp}\"\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"ðŸ“ Creating results directory: {results_dir}\")\n",
    "    \n",
    "    # Export write performance\n",
    "    if 'write_results' in locals() and write_results:\n",
    "        write_filename = os.path.join(results_dir, f\"write_performance.json\")\n",
    "        with open(write_filename, 'w') as f:\n",
    "            json.dump(write_results, f, indent=2, default=str)\n",
    "        print(f\"âœ“ Write performance saved to: {write_filename}\")\n",
    "    \n",
    "    # Export read benchmarks\n",
    "    if 'benchmark_results' in locals() and benchmark_results:\n",
    "        read_filename = os.path.join(results_dir, f\"read_benchmarks.json\")\n",
    "        with open(read_filename, 'w') as f:\n",
    "            json.dump(benchmark_results, f, indent=2, default=str)\n",
    "        print(f\"âœ“ Read benchmarks saved to: {read_filename}\")\n",
    "    \n",
    "    # Export session summary with cluster configuration and run metadata\n",
    "    summary = {\n",
    "        \"run_metadata\": {\n",
    "            \"run_name\": RUN_NAME,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"session_start\": session_start.isoformat(),\n",
    "            \"session_end\": session_end.isoformat(),\n",
    "            \"duration_seconds\": session_duration.total_seconds()\n",
    "        },\n",
    "        \"configuration\": {\n",
    "            \"schema\": SCHEMA,\n",
    "            \"table_name\": TABLE_NAME,\n",
    "            \"cluster_id\": CLUSTER_ID,\n",
    "            \"use_row_count\": USE_ROW_COUNT,\n",
    "            \"num_rows\": NUM_ROWS if USE_ROW_COUNT else None,\n",
    "            \"target_size_gb\": TARGET_SIZE_GB if not USE_ROW_COUNT else None,\n",
    "            \"text_length\": TEXT_LENGTH\n",
    "        },\n",
    "        \"cluster_configuration\": cluster_configuration if 'cluster_configuration' in locals() else None,\n",
    "        \"performance_summary\": {\n",
    "            \"write_throughput_rows_per_sec\": write_results.get('overall_rows_per_second') if 'write_results' in locals() and write_results else None,\n",
    "            \"write_throughput_mb_per_sec\": write_results.get('overall_mb_per_second') if 'write_results' in locals() and write_results else None,\n",
    "            \"avg_read_query_time_sec\": sum(r[\"execution_time_seconds\"] for r in benchmark_results if r[\"status\"] == \"success\") / len([r for r in benchmark_results if r[\"status\"] == \"success\"]) if 'benchmark_results' in locals() and benchmark_results else None,\n",
    "            \"successful_read_queries\": len([r for r in benchmark_results if r[\"status\"] == \"success\"]) if 'benchmark_results' in locals() and benchmark_results else 0,\n",
    "            \"total_read_queries\": len(benchmark_results) if 'benchmark_results' in locals() and benchmark_results else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_filename = os.path.join(results_dir, f\"session_summary.json\")\n",
    "    with open(summary_filename, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    print(f\"âœ“ Session summary saved to: {summary_filename}\")\n",
    "    \n",
    "    # Create a simple README for the run\n",
    "    readme_content = f\"\"\"# Benchmarking Run: {RUN_NAME}\n",
    "\n",
    "## Run Information\n",
    "- **Run Name**: {RUN_NAME}\n",
    "- **Timestamp**: {timestamp}\n",
    "- **Date**: {session_start.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Duration**: {session_duration}\n",
    "\n",
    "## Configuration\n",
    "- **Table**: {SCHEMA}.{TABLE_NAME}\n",
    "- **Cluster**: {CLUSTER_ID}\n",
    "{\"- **Rows**: {:,}\".format(NUM_ROWS) if USE_ROW_COUNT else \"- **Target Size**: {} GB\".format(TARGET_SIZE_GB)}\n",
    "- **Text Length**: {TEXT_LENGTH:,} bytes\n",
    "\n",
    "## Files in this directory\n",
    "- `write_performance.json` - Detailed write performance metrics\n",
    "- `read_benchmarks.json` - Read query benchmark results  \n",
    "- `session_summary.json` - Complete session summary with cluster info\n",
    "- `README.md` - This file\n",
    "\n",
    "## Quick Results Summary\n",
    "\"\"\"\n",
    "    \n",
    "    if 'write_results' in locals() and write_results:\n",
    "        readme_content += f\"- **Write Throughput**: {write_results['overall_rows_per_second']:.1f} rows/s\\n\"\n",
    "        readme_content += f\"- **Data Size**: {write_results['total_data_size_mb']:.2f} MB\\n\"\n",
    "    \n",
    "    if 'benchmark_results' in locals() and benchmark_results:\n",
    "        successful_reads = [r for r in benchmark_results if r[\"status\"] == \"success\"]\n",
    "        if successful_reads:\n",
    "            avg_read_time = sum(r[\"execution_time_seconds\"] for r in successful_reads) / len(successful_reads)\n",
    "            readme_content += f\"- **Average Read Query Time**: {avg_read_time:.3f}s\\n\"\n",
    "            readme_content += f\"- **Successful Queries**: {len(successful_reads)}/{len(benchmark_results)}\\n\"\n",
    "    \n",
    "    readme_filename = os.path.join(results_dir, \"README.md\")\n",
    "    with open(readme_filename, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"âœ“ Run documentation saved to: {readme_filename}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š All results exported to: {results_dir}\")\n",
    "    print(f\"ðŸ·ï¸  Run identifier: {RUN_NAME}_{timestamp}\")\n",
    "else:\n",
    "    print(\"Results not exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### To run different configurations:\n",
    "1. Modify the configuration variables in the \"Configuration\" section\n",
    "2. Re-run the \"Create Dataset\" and \"Benchmark Queries\" sections\n",
    "\n",
    "### To compare results:\n",
    "- Export results from different runs using different configurations\n",
    "- Analyze JSON files to compare performance across different table sizes\n",
    "\n",
    "### Recommended experiments:\n",
    "- Test different row counts (1K, 10K, 100K, 1M)\n",
    "- Test different text lengths (1KB, 10KB, 100KB, 1MB)\n",
    "- Compare performance with different cluster configurations\n",
    "- Analyze query patterns that perform best/worst on your data\n",
    "\n",
    "### Performance optimization:\n",
    "- Monitor cluster utilization during tests\n",
    "- Consider partitioning strategies for larger datasets\n",
    "- Test with different Spark configurations\n",
    "- Analyze query execution plans for optimization opportunities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}